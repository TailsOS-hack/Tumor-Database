{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI-Powered Brain Tumor & Dementia Training Suite**\n",
    "\n",
    "This notebook trains three deep learning models:",
    "1.  **Gatekeeper:** A ResNet50 classifier (Normal vs. Tumor vs. Dementia).\n",
    "2.  **Tumor Specialist:** An EfficientNet-B3 model (Glioma vs. Meningioma vs. Pituitary).\n",
    "3.  **Dementia Specialist:** A MobileNetV3 model (Mild vs. Moderate vs. Very Mild).\n",
    "\n",
    "## **Instructions**\n",
    "1.  **Upload Data:** Upload the `data.zip` file generated by the CLI tool to the Files tab on the left.\n",
    "2.  **Runtime:** Go to `Runtime` -> `Change runtime type` -> Select `T4 GPU`.\n",
    "3.  **Run All:** Click `Runtime` -> `Run all`.\n",
    "4.  **Download:** Once finished, download the `trained_models.zip` file containing your new models and training graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: GPU not found. Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed)\n",
    "!pip install torch torchvision matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Data\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    if os.path.exists('data.zip'):\n",
    "        print(\"Unzipping data.zip...\")\n",
    "        with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(\"Data extracted successfully.\")\n",
    "    else:\n",
    "        print(\"ERROR: 'data.zip' not found. Please upload it to the Files tab.\")\n",
    "else:\n",
    "    print(\"Data folder already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gatekeeper Model (Inline)\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class GatekeeperClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, freeze_base=True):\n",
    "        super(GatekeeperClassifier, self).__init__()\n",
    "        try:\n",
    "            self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        except:\n",
    "            self.base_model = models.resnet50(pretrained=True)\n",
    "\n",
    "        if freeze_base:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Script (Inline)\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "IMG_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "LOGS_DIR = 'training_logs'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset Class\n",
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {path}: {e}\")\n",
    "            return torch.zeros((3, IMG_SIZE, IMG_SIZE)), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Helpers\n",
    "def format_time(seconds):\n",
    "    return str(datetime.timedelta(seconds=int(seconds)))\n",
    "\n",
    "def get_transforms():\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(LOGS_DIR, f'{model_name.replace(\" \", \"_\")}_loss.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_acc'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(LOGS_DIR, f'{model_name.replace(\" \", \"_\")}_accuracy.png'))\n",
    "    plt.close()\n",
    "\n",
    "def train_and_save(model, train_loader, val_loader, save_path, num_classes, model_name):\n",
    "    logger.info(f\"STARTING TRAINING: {model_name} on {DEVICE}\")\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch+1}/{NUM_EPOCHS}: Train Loss={epoch_loss:.4f}, Acc={epoch_acc:.4f} | Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path) # Saving state dict is safer for portability\n",
    "            logger.info(f\"New best model saved! (Acc: {best_acc:.4f})\")\n",
    "\n",
    "    with open(os.path.join(LOGS_DIR, f'{model_name.replace(\" \", \"_\")}_history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "    plot_training_history(history, model_name)\n",
    "\n",
    "def gather_files():\n",
    "    def get_files(pattern):\n",
    "        return glob.glob(os.path.join(DATA_ROOT, pattern), recursive=True)\n",
    "\n",
    "    bt_train = os.path.join('brain_tumor', 'Training')\n",
    "    bt_test = os.path.join('brain_tumor', 'Testing')\n",
    "    glioma = get_files(os.path.join(bt_train, 'glioma', '*')) + get_files(os.path.join(bt_test, 'glioma', '*'))\n",
    "    meningioma = get_files(os.path.join(bt_train, 'meningioma', '*')) + get_files(os.path.join(bt_test, 'meningioma', '*'))\n",
    "    pituitary = get_files(os.path.join(bt_train, 'pituitary', '*')) + get_files(os.path.join(bt_test, 'pituitary', '*'))\n",
    "    tumor_normal = get_files(os.path.join(bt_train, 'notumor', '*')) + get_files(os.path.join(bt_test, 'notumor', '*'))\n",
    "    \n",
    "    alz = 'alzheimers'\n",
    "    mild = get_files(os.path.join(alz, 'MildDemented', '*'))\n",
    "    moderate = get_files(os.path.join(alz, 'ModerateDemented', '*'))\n",
    "    very_mild = get_files(os.path.join(alz, 'VeryMildDemented', '*'))\n",
    "    alz_normal = get_files(os.path.join(alz, 'NonDemented', '*'))\n",
    "    return {'glioma': glioma, 'meningioma': meningioma, 'pituitary': pituitary, 'tumor_normal': tumor_normal, 'mild': mild, 'moderate': moderate, 'very_mild': very_mild, 'alz_normal': alz_normal}\n",
    "\n",
    "# Execution Block\n",
    "files = gather_files()\n",
    "train_tf, val_tf = get_transforms()\n",
    "\n",
    "# 1. Gatekeeper\n",
    "gk_paths = files['tumor_normal'] + files['alz_normal'] + files['glioma'] + files['meningioma'] + files['pituitary'] + files['mild'] + files['moderate'] + files['very_mild']\n",
    "gk_labels = [0]*len(files['tumor_normal'] + files['alz_normal']) + [1]*len(files['glioma'] + files['meningioma'] + files['pituitary']) + [2]*len(files['mild'] + files['moderate'] + files['very_mild'])\n",
    "train_idx, val_idx = random_split(range(len(gk_paths)), [int(0.8*len(gk_paths)), len(gk_paths)-int(0.8*len(gk_paths))])\n",
    "train_loader = DataLoader(MedicalImageDataset([gk_paths[i] for i in train_idx.indices], [gk_labels[i] for i in train_idx.indices], transform=train_tf), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(MedicalImageDataset([gk_paths[i] for i in val_idx.indices], [gk_labels[i] for i in val_idx.indices], transform=val_tf), batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_and_save(GatekeeperClassifier(num_classes=3), train_loader, val_loader, os.path.join(MODELS_DIR, 'gatekeeper_classifier.pt'), 3, \"Gatekeeper\")\n",
    "\n",
    "# 2. Tumor\n",
    "tm_paths = files['glioma'] + files['meningioma'] + files['pituitary']\n",
    "tm_labels = [0]*len(files['glioma']) + [1]*len(files['meningioma']) + [2]*len(files['pituitary'])\n",
    "train_idx, val_idx = random_split(range(len(tm_paths)), [int(0.8*len(tm_paths)), len(tm_paths)-int(0.8*len(tm_paths))])\n",
    "train_loader = DataLoader(MedicalImageDataset([tm_paths[i] for i in train_idx.indices], [tm_labels[i] for i in train_idx.indices], transform=train_tf), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(MedicalImageDataset([tm_paths[i] for i in val_idx.indices], [tm_labels[i] for i in val_idx.indices], transform=val_tf), batch_size=BATCH_SIZE, shuffle=False)\n",
    "tm_model = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "tm_model.classifier[1] = nn.Linear(tm_model.classifier[1].in_features, 3)\n",
    "train_and_save(tm_model, train_loader, val_loader, os.path.join(MODELS_DIR, 'brain_tumor_classifier.pt'), 3, \"Tumor Specialist\")\n",
    "\n",
    "# 3. Dementia\n",
    "dm_paths = files['mild'] + files['moderate'] + files['very_mild']\n",
    "dm_labels = [0]*len(files['mild']) + [1]*len(files['moderate']) + [2]*len(files['very_mild'])\n",
    "train_idx, val_idx = random_split(range(len(dm_paths)), [int(0.8*len(dm_paths)), len(dm_paths)-int(0.8*len(dm_paths))])\n",
    "train_loader = DataLoader(MedicalImageDataset([dm_paths[i] for i in train_idx.indices], [dm_labels[i] for i in train_idx.indices], transform=train_tf), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(MedicalImageDataset([dm_paths[i] for i in val_idx.indices], [dm_labels[i] for i in val_idx.indices], transform=val_tf), batch_size=BATCH_SIZE, shuffle=False)\n",
    "dm_model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)\n",
    "dm_model.classifier[3] = nn.Linear(dm_model.classifier[3].in_features, 3)\n",
    "train_and_save(dm_model, train_loader, val_loader, os.path.join(MODELS_DIR, 'alzheimers_classifier.pt'), 3, \"Dementia Specialist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip Results\n",
    "!zip -r trained_models.zip models/ training_logs/\n",
    "from google.colab import files\n",
    "files.download('trained_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
